{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Pattern Recognition - Solution 11: Sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import time\n",
    "from matplotlib.patches import Ellipse\n",
    "import scipy.stats\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# ensure that rerunning the notebook will give fixed results\n",
    "np.random.seed(2701)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian(ax, mean, cov, color='red', size=3):\n",
    "    # draws ellipses representing different standard deviations (size is the number of ellipses)\n",
    "    # width and height are times by 2 since sqrt of the eigenval only measures half the distance\n",
    "    eig_w, eig_v = np.linalg.eig(cov)\n",
    "    # eigenvectors are orthogonal so we only need to check the first one\n",
    "    eig_v_0 = eig_v[0]\n",
    "    # arccos of first eigenvectors x coordinate will give us the angle\n",
    "    angle = np.arccos(eig_v_0[0])\n",
    "    # however, this only works for angles in [0, pi]\n",
    "    # for angles in [pi, 2*pi] we need to consider the\n",
    "    # y coordinate, too\n",
    "    if eig_v_0[1] > 0:\n",
    "        angle = 2 * np.pi - angle\n",
    "    angle = np.rad2deg(angle)\n",
    "\n",
    "    for i in range(size):\n",
    "        ell = Ellipse(xy=[mean[0], mean[1]],\n",
    "                      width=np.sqrt(eig_w[0]) * 2 * (i + 1),\n",
    "                      height=np.sqrt(eig_w[1]) * 2 * (i + 1),\n",
    "                      angle=angle,\n",
    "                      edgecolor=color, lw=2, facecolor='none')\n",
    "        ax.add_artist(ell)\n",
    "    print(mean)\n",
    "    ax.plot(mean[0], mean[1], \"x\", c=color, ms=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\star$ Part 1: Box-Muller method\n",
    "\n",
    "Implement the Box-Muller method to produce $N$ samples from an isotropic Gaussian distribution of dimension $D$ with mean $\\mu \\in \\mathbb{R}^D$  and covariance $\\sigma^2 I \\in \\mathbb{R}^{D \\times D}$ given a uniform proposal distribution. $I$ is the identity matrix. See [wikipedia](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) for the math. Use [np.random.uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) for sampling the uniform distribution.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* In a multivariate isotropic Gaussian the dimensions are independent of each other and can therefore be sampled independently.\n",
    "* The Box-Muller method will give you a standard Gaussian distribution with mean 0 and standard deviation 1. Scale it by the standard deviation and add the mean to get an arbitrary Gaussian distribution.\n",
    "\n",
    "Plot some samples and visualize the standard deviation with the utility function `plot_gaussian` above.\n",
    "\n",
    "For comparison, plot samples with [scipy.stats.multivariate_normal.rvs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def box_muller(n_samples:int, dim:int, mean:np.ndarray, std:float):\n",
    "    # START TODO ################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "    return samples\n",
    "\n",
    "\n",
    "# test function with 11 samples of a 3d gaussian\n",
    "gaussian_samples_3d = box_muller(11, 3, np.array([0., 0., 0.]), 1.)\n",
    "assert gaussian_samples_3d.shape == (11, 3)\n",
    "\n",
    "# define 2d gaussian\n",
    "mean_2d = np.array([10., 20.])\n",
    "std_2d = 5.\n",
    "n = 10000\n",
    "\n",
    "# START TODO ################\n",
    "# sample and plot the samples\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "# START TODO ################\n",
    "# plot the standard deviation ellipsis\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# sample and plot the same gaussian using scipy\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\star$ $\\star$ Part 2: Metropolis algorithm\n",
    "\n",
    "Implement the Metropolis algorithm to sample from an anisotropic Gaussian distribution via an isotropic Gaussian proposal distribution. Use the box-muller method from above to sample from the proposal distribution.\n",
    "\n",
    "Mean $\\mu$ and covariance $\\Sigma$ of the anisotropic Gaussian are given below (note that the covariance is and must be nonsingular). You can create an anisotropic gaussian object `scipy.stats.multivariate_normal` and then use its function `pdf(x)` to get the probability density at point `x`. Note that here the probability $p(x)$ is given, however for the Metropolis algorithm an unnormalized function $f(x) = p(x) / Z$ with an unknown normalization constant $Z$ would work as well. This is one of the advantages of the algorithm as $Z$ can be hard or impossible to compute depending on the distribution.\n",
    "\n",
    "Choose the proposal distribution as $q(z|z^{(l)}) = \\mathcal{N}(z^{(l)}, \\sigma_q^2 I )$ and $q(z) = \\mathcal{N}(\\mu_q, \\sigma_q^2 I)$ and set the hyperparameters as $\\mu_q = \\mu$, $\\sigma_q = 0.1$.\n",
    "\n",
    "Produce 10000 samples and plot them and the standard deviation ellipses. Note that you will need more trials than the number of samples since some samples will be discarded. You can safeguard against division by zero to avoid warnings.\n",
    "\n",
    "Aside from comparing the samples of the distributions visually, compute the maximum-likelihood estimate of mean and covariance from the samples as in exercise 2 with `np.mean(samples, axis=0)` and `np.cov(samples.T)` and plot the standard deviation ellipses of your fit. You could then also evaluate the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence) between the original gaussian and the gaussian estimated from the samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([-1.26745097,  0.1375625 ])\n",
    "cov = np.array([[3.10419125, 1.61656817], [1.61656817, 1.2180084 ]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = scipy.stats.multivariate_normal(mean, cov)\n",
    "\n",
    "def get_kldiv_of_gaussians(m0, c0, m1, c1):\n",
    "    # START TODO ################\n",
    "    # compute the KL divergence between two multivariate gaussians\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "    return kldiv\n",
    "\n",
    "def compare_estimated_gaussian(ax, samples, orig_mean, orig_cov):\n",
    "    # plot the estimated gaussian on the given axes and compare it to the original one\n",
    "\n",
    "    # START TODO ################\n",
    "    # estimate gaussian from samples\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "\n",
    "    # START TODO ################\n",
    "    # plot standard deviation ellipses\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "\n",
    "    # START TODO ################\n",
    "    # compute kl-divergence\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "    print(f\"KL-Divergence D_kl(p || p_est): {kl}\")\n",
    "\n",
    "\n",
    "mean0 = mean # np.array([0, 0])\n",
    "std = 0.1\n",
    "n_samples = 10000\n",
    "max_tries = 20000\n",
    "dim = 2\n",
    "\n",
    "t1 = timer()\n",
    "\n",
    "# compute the random variables at once before\n",
    "us = np.random.randn(max_tries)\n",
    "steps = box_muller(max_tries, dim, np.zeros(2), std)\n",
    "\n",
    "# START TODO ################\n",
    "# create the initial sample\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "zs = []\n",
    "# START TODO ################\n",
    "# implement the Metropolis algorithm\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "print(f\"Found {len(zs)} samples in {n} trials after {timer() - t1:.3f}sec.\")\n",
    "samples = np.array(zs)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot()\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.scatter(samples[:, 0], samples[:, 1])\n",
    "plot_gaussian(ax, mean, cov)\n",
    "compare_estimated_gaussian(ax, samples, mean, cov)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results with directly sampling from the anisotropic Gaussian with its method `rvs`. What do you observe about the quality of the approximation? Can you improve it? Also, the master solution takes about 0.5 second to produce 10000 samples. Can you match this performance or even create a faster implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gaussian.rvs(size=n_samples)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot()\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.scatter(samples[:, 0], samples[:, 1])\n",
    "plot_gaussian(ax, mean, cov)\n",
    "compare_estimated_gaussian(ax, samples, mean, cov)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments to improve the algorithm\n",
    "\n",
    "* To keep the samples from going too far away from the mean, restart the algorithm every `S` samples.\n",
    "* Now it's possible to increase the standard deviation of the proposal distribution without the samples diverging too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}